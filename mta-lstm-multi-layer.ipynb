{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTA-LSTM-PyTorch\n",
    "\n",
    "This is an implementation of the paper [Topic-to-Essay Generation with Neural Networks](http://ir.hit.edu.cn/~xcfeng/xiaocheng%20Feng's%20Homepage_files/final-topic-essay-generation.pdf). The original work can be found [here](https://github.com/hit-computer/MTA-LSTM), which is implemented in TensorFlow and is totally out-of-date, further more, the owner doesn't seem to maintain it anymore. Therefore, I decided to re-implement it in a simple yet powerful framework, PyTorch.\n",
    "\n",
    "In this notebook, I'll show you how to build a neural network proposed in the paper step by step from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "The followings are some packages that'll be used in this work. Make sure you have them installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, autograd, optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import Parameter, LayerNorm\n",
    "from torch.autograd import Variable\n",
    "import torch.jit as jit\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "from collections import namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# !wget \"https://noto-website-2.storage.googleapis.com/pkgs/NotoSansCJKtc-hinted.zip\"\n",
    "# !unzip NotoSansCJKtc-hinted.zip\n",
    "# !sudo mv NotoSansCJKtc-Black.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Bold.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-DemiLight.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Light.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Medium.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Regular.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansCJKtc-Thin.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansMonoCJKtc-Bold.otf /usr/share/fonts/truetype/\n",
    "# !sudo mv NotoSansMonoCJKtc-Regular.otf /usr/share/fonts/truetype/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "font_dirs = ['/fonts/', ]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs)\n",
    "font_list = font_manager.createFontList(font_files)\n",
    "font_manager.fontManager.ttflist.extend(font_list)\n",
    "path = 'NotoSansCJKtc-Regular.otf'\n",
    "fontprop = fm.FontProperties(fname=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Available cuda: 1\nCurrent device: 0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "print('Available cuda:', torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    # Consider to change device_num to match with your machine\n",
    "    device_num = 0\n",
    "    deviceName = \"cuda:%d\" % device_num\n",
    "    torch.cuda.set_device(device_num)\n",
    "    print('Current device:', torch.cuda.current_device())\n",
    "else:\n",
    "    deviceName = \"cpu\"\n",
    "    \n",
    "device = torch.device(deviceName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dictionary and pretrained embedding system\n",
    "\n",
    "Here I'm gonna load the pretrained word2vec vocab and vectors. Please refer to [this notebook]() to he how to train it.\n",
    "\n",
    "The code ```fvec.vectors``` is where we get the pretrained vectors.\n",
    "```<PAD>```, ```<BOS>```, ```<EOS>``` and ```<UNK>``` are 4 common tokens which stands for **PADding**, **Begin-Of-Sentence**, **End-Of-Sentence** and **UNKnown** respectively. We simply add them into the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total 196849 words\n"
     ]
    }
   ],
   "source": [
    "file_path = 'data/'\n",
    "# Pls run this file \\data\\Word2Vec.ipynb to generate file composition_mincount_1_305000_vec_original.txt\n",
    "fvec = KeyedVectors.load_word2vec_format(file_path+'composition_mincount_1_305000_vec_original.txt', binary=False)\n",
    "word_vec = fvec.vectors\n",
    "vocab = ['<PAD>', '<BOS>', '<EOS>', '<UNK>']\n",
    "vocab.extend(list(fvec.vocab.keys()))\n",
    "word_vec = np.concatenate((np.array([[0]*word_vec.shape[1]] * 4), word_vec))\n",
    "word_vec = torch.tensor(word_vec).float()\n",
    "del fvec\n",
    "print(\"total %d words\" % len(word_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please create folder: model_result_multi_layer\n",
    "save_folder = 'model_result_multi_layer'\n",
    "vocab_check_point = '%s/vocab.pkl' % save_folder\n",
    "word_vec_check_point = '%s/word_vec.pkl' % save_folder\n",
    "torch.save(vocab, vocab_check_point)\n",
    "torch.save(word_vec, word_vec_check_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a word-index convertor\n",
    "\n",
    "We don't want to use type of string directly when training, instead we map them to a unique index in integer. In text generation phase, we'll then convert them back to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx_to_word = {i: ch for i, ch in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data\n",
    "\n",
    "You can prepare for your own data, or simply use what I offered in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6857/6857 [00:00<00:00, 32651.45it/s]\n"
     ]
    }
   ],
   "source": [
    "essays = []\n",
    "topics = []\n",
    "# num_lines = sum(1 for line in open(file_path+'composition_zh_tw.txt', 'r'))\n",
    "news_filepath = 'news_data.txt'\n",
    "num_lines = sum(1 for line in open(file_path+ news_filepath, 'r'))\n",
    "# with open(file_path+'composition_zh_tw.txt') as f:\n",
    "with open(file_path+ news_filepath) as f:\n",
    "    for line in tqdm(f, total=num_lines):\n",
    "        essay, topic = line.replace('\\n', '').split(' </d> ')\n",
    "        essays.append(essay.split(' '))\n",
    "        topics.append(topic.split(' '))\n",
    "    f.close()\n",
    "    \n",
    "assert len(topics) == len(essays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then map all the training and testing corpus to integer index word-by-word, with the help of our convertor. Note that we map it to ```<UNK>``` if the words in corpus are not in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 6857/6857 [00:00<00:00, 15126.78it/s]\n",
      "100%|██████████| 6857/6857 [00:00<00:00, 253967.91it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_indice = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(essays[:300000])))\n",
    "topics_indice = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(topics[:300000])))\n",
    "corpus_test = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(essays[300000:305000])))\n",
    "topics_test = list(map(lambda x: [word_to_idx[w] if (w in word_to_idx) else word_to_idx['<UNK>'] for w in x], tqdm(topics[300000:305000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viewData(topics, X):\n",
    "    topics = [idx_to_word[x] for x in topics]\n",
    "    X = [idx_to_word[x] for x in X]\n",
    "    print(topics, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def shuffleData(topics_indice, corpus_indice):\n",
    "    ind_list = [i for i in range(len(topics_indice))]\n",
    "    shuffle(ind_list)\n",
    "    topics_indice = np.array(topics_indice)\n",
    "    corpus_indice = np.array(corpus_indice)\n",
    "    topics_indice = topics_indice[ind_list,]\n",
    "    corpus_indice = corpus_indice[ind_list,]\n",
    "    topics_indice = topics_indice.tolist()\n",
    "    corpus_indice = corpus_indice.tolist()\n",
    "    return topics_indice, corpus_indice\n",
    "\n",
    "# topics_indice, corpus_indice = shuffleData(topics_indice, corpus_indice)\n",
    "# viewData(topics_indice[0], corpus_indice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n",
      "less than 5\n"
     ]
    }
   ],
   "source": [
    "for t in topics_indice:\n",
    "    if len(t) != 5:\n",
    "        print('less than 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know the max length of training corpus too, in order to pad sequences that aren't long enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = list(map(lambda x: len(x), corpus_indice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete useless dependencies to free up some space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "del essays\n",
    "del topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch data iterator\n",
    "\n",
    "We want to iter through training data in batches and feed them into the network, and this is how we prepare for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator(corpus_indice, topics_indice, batch_size, num_steps):\n",
    "    epoch_size = len(corpus_indice) // batch_size\n",
    "    for i in range(epoch_size):\n",
    "        raw_data = corpus_indice[i*batch_size: (i+1)*batch_size]\n",
    "        key_words = topics_indice[i*batch_size: (i+1)*batch_size]\n",
    "        data = np.zeros((len(raw_data), num_steps+1), dtype=np.int64)\n",
    "        for i in range(batch_size):\n",
    "            doc = raw_data[i]\n",
    "            tmp = [1]\n",
    "            tmp.extend(doc)\n",
    "            tmp.extend([2])\n",
    "            tmp = np.array(tmp, dtype=np.int64)\n",
    "            _size = tmp.shape[0]\n",
    "            data[i][:_size] = tmp\n",
    "        key_words = np.array(key_words, dtype=np.int64)\n",
    "        x = data[:, 0:num_steps]\n",
    "        y = data[:, 1:]\n",
    "        mask = np.float32(x != 0)\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "        mask = torch.tensor(mask)\n",
    "        key_words = torch.tensor(key_words)\n",
    "        yield(x, y, mask, key_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model: MTA-LSTM\n",
    "\n",
    "This is the most important part in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Implements Bahdanau (MLP) attention\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, embed_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.Ua = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.va = nn.Linear(hidden_size, 1, bias=True)\n",
    "        # to store attention scores\n",
    "        self.alphas = None\n",
    "        \n",
    "    def forward(self, query, topics, coverage_vector):\n",
    "        scores = []\n",
    "        C_t = coverage_vector.clone()\n",
    "        for i in range(topics.shape[1]):\n",
    "            proj_key = self.Ua(topics[:, i, :])\n",
    "            query = self.Wa(query)\n",
    "            scores += [self.va(torch.tanh(query + proj_key)) * C_t[:, i:i+1]]\n",
    "            \n",
    "        # stack scores\n",
    "        scores = torch.stack(scores, dim=1)\n",
    "        scores = scores.squeeze(2)\n",
    "#         print(scores.shape)\n",
    "        # turn scores to probabilities\n",
    "        alphas = F.softmax(scores, dim=1)\n",
    "        self.alphas = alphas\n",
    "        \n",
    "        # mt vector is the weighted sum of the topics\n",
    "        mt = torch.bmm(alphas.unsqueeze(1), topics)\n",
    "        mt = mt.squeeze(1)\n",
    "        \n",
    "        # mt shape: [batch x embed], alphas shape: [batch x num_keywords]\n",
    "        return mt, alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, embed_size, num_layers, dropout=0.5):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # topic attention\n",
    "        self.attention = Attention(hidden_size, embed_size)\n",
    "        \n",
    "        # lstm\n",
    "        self.rnn = nn.LSTM(input_size=embed_size * 2, \n",
    "                           hidden_size=hidden_size, \n",
    "                           num_layers=num_layers, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "    def forward(self, input, output, hidden, phi, topics, coverage_vector):\n",
    "        # 1. calculate attention weight and mt\n",
    "        mt, score = self.attention(output.squeeze(0), topics, coverage_vector)\n",
    "        mt = mt.unsqueeze(1).permute(1, 0, 2)\n",
    "        \n",
    "        # 2. update coverge vector [batch x num_keywords]\n",
    "        coverage_vector = coverage_vector - score / phi\n",
    "        \n",
    "        # 3. concat input and Tt, and feed into rnn \n",
    "        output, hidden = self.rnn(torch.cat([input, mt], dim=2), hidden)\n",
    "        \n",
    "        return output, hidden, score, coverage_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MTA-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMState = namedtuple('LSTMState', ['hx', 'cx'])\n",
    "class MTALSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, embed_dim, num_keywords, num_layers, weight,\n",
    "                 num_labels, bidirectional, dropout=0.5, **kwargs):\n",
    "        super(MTALSTM, self).__init__(**kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_labels = num_labels\n",
    "        self.bidirectional = bidirectional\n",
    "        if num_layers <= 1:\n",
    "            self.dropout = 0\n",
    "        else:\n",
    "            self.dropout = dropout\n",
    "        self.embedding = nn.Embedding.from_pretrained(weight)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.Uf = nn.Linear(embed_dim * num_keywords, num_keywords, bias=False)\n",
    "        \n",
    "        # attention decoder\n",
    "        self.decoder = AttentionDecoder(hidden_size=hidden_dim, \n",
    "                                        embed_size=embed_dim, \n",
    "                                        num_layers=num_layers, \n",
    "                                        dropout=dropout)\n",
    "        \n",
    "        # adaptive softmax\n",
    "        self.adaptiveSoftmax = nn.AdaptiveLogSoftmaxWithLoss(hidden_dim, \n",
    "                                                             num_labels, \n",
    "                                                             cutoffs=[round(num_labels / 20), 4*round(num_labels / 20)])\n",
    "    \n",
    "    def forward(self, inputs, topics, output, hidden=None, mask=None, target=None, coverage_vector=None, seq_length=None):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        topics_embed = self.embedding(topics)\n",
    "        ''' calculate phi [batch x num_keywords] '''\n",
    "        phi = None\n",
    "        phi = torch.sum(mask, dim=1, keepdim=True) * torch.sigmoid(self.Uf(topics_embed.reshape(topics_embed.shape[0], -1).float()))\n",
    "        \n",
    "        # loop through sequence\n",
    "        inputs = embeddings.permute([1, 0, 2]).unbind(0)\n",
    "        output_states = []\n",
    "        attn_weight = []\n",
    "        for i in range(len(inputs)):\n",
    "            output, hidden, score, coverage_vector = self.decoder(input=inputs[i].unsqueeze(0), \n",
    "                                                                        output=output, \n",
    "                                                                        hidden=hidden, \n",
    "                                                                        phi=phi, \n",
    "                                                                        topics=topics_embed, \n",
    "                                                                        coverage_vector=coverage_vector) # [seq_len x batch x embed_size]\n",
    "            output_states += [output]\n",
    "            attn_weight += [score]\n",
    "            \n",
    "        output_states = torch.stack(output_states)\n",
    "        attn_weight = torch.stack(attn_weight)\n",
    "        \n",
    "        # calculate loss py adaptiveSoftmax\n",
    "        outputs = self.adaptiveSoftmax(output_states.reshape(-1, output_states.shape[-1]), target.t().reshape((-1,)))\n",
    "        \n",
    "        return outputs, output_states, hidden, attn_weight, coverage_vector\n",
    "    \n",
    "    def inference(self, inputs, topics, output, hidden=None, mask=None, coverage_vector=None, seq_length=None):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        topics_embed = self.embedding(topics)\n",
    "       \n",
    "        phi = None\n",
    "        phi = seq_length.float() * torch.sigmoid(self.Uf(topics_embed.reshape(topics_embed.shape[0], -1).float()))\n",
    "        \n",
    "        queries = embeddings.permute([1, 0, 2])[-1].unsqueeze(0)\n",
    "        \n",
    "        inputs = queries.permute([1, 0, 2]).unbind(0)\n",
    "        output_states = []\n",
    "        attn_weight = []\n",
    "        for i in range(len(inputs)):\n",
    "            output, hidden, score, coverage_vector = self.decoder(input=inputs[i].unsqueeze(0), \n",
    "                                                                        output=output, \n",
    "                                                                        hidden=hidden, \n",
    "                                                                        phi=phi, \n",
    "                                                                        topics=topics_embed, \n",
    "                                                                        coverage_vector=coverage_vector) # [seq_len x batch x embed_size]\n",
    "            output_states += [output]\n",
    "            attn_weight += [score]\n",
    "            \n",
    "        output_states = torch.stack(output_states)\n",
    "        attn_weight = torch.stack(attn_weight)\n",
    "        \n",
    "        outputs = self.adaptiveSoftmax.log_prob(output_states.reshape(-1, output_states.shape[-1]))\n",
    "        return outputs, output_states, hidden, attn_weight, coverage_vector\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "#         hidden = torch.zeros(num_layers, batch_size, hidden_dim)\n",
    "#         hidden = LSTMState(torch.zeros(batch_size, hidden_dim).to(device), torch.zeros(batch_size, hidden_dim).to(device))\n",
    "        hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device), \n",
    "                  torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
    "        return hidden\n",
    "    \n",
    "    def init_coverage_vector(self, batch_size, num_keywords):\n",
    "#         self.coverage_vector = torch.ones([batch_size, num_keywords]).to(device)\n",
    "        return torch.ones([batch_size, num_keywords]).to(device)\n",
    "#         print(self.coverage_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy decode strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_topic(topics):\n",
    "    topics = [word_to_idx[x] for x in topics]\n",
    "    topics = torch.tensor(topics)\n",
    "    print(topics)\n",
    "    max_num = 5\n",
    "    size = 1\n",
    "    ans = np.zeros((size, max_num), dtype=int)\n",
    "    for i in range(size):\n",
    "        true_len = min(len(topics), max_num)\n",
    "        for j in range(true_len):\n",
    "            print(topics[i])\n",
    "            ans[i][j] = topics[i][j]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(topics, num_chars, model, idx_to_word, word_to_idx):\n",
    "    output_idx = [1]\n",
    "    topics = [word_to_idx[x] for x in topics]\n",
    "    topics = torch.tensor(topics)\n",
    "    topics = topics.reshape((1, topics.shape[0]))\n",
    "#     hidden = torch.zeros(num_layers, 1, hidden_dim)\n",
    "#     hidden = (torch.zeros(num_layers, 1, hidden_dim).to(device), torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    if use_gpu:\n",
    "#         hidden = hidden.cuda()\n",
    "        adaptive_softmax.to(device)\n",
    "        topics = topics.to(device)\n",
    "    coverage_vector = model.init_coverage_vector(topics.shape[0], topics.shape[1])\n",
    "    attentions = torch.zeros(num_chars, topics.shape[1])\n",
    "    for t in range(num_chars):\n",
    "        X = torch.tensor(output_idx[-1]).reshape((1, 1))\n",
    "#         X = torch.tensor(output).reshape((1, len(output)))\n",
    "        if use_gpu:\n",
    "            X = X.to(device)\n",
    "        if t == 0:\n",
    "            output = torch.zeros(1, hidden_dim).to(device)\n",
    "        else:\n",
    "            output = output.squeeze(0)\n",
    "        pred, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, topics=topics, output=output, hidden=hidden, coverage_vector=coverage_vector, seq_length=torch.tensor(50).reshape(1, 1).to(device))\n",
    "#         print(coverage_vector)\n",
    "        pred = pred.argmax(dim=1) # greedy strategy\n",
    "        attentions[t] = attn_weight[0].data\n",
    "#         pred = adaptive_softmax.predict(pred)\n",
    "        if pred[-1] == 2:\n",
    "#         if pred.argmax(dim=1)[-1] == 2:\n",
    "            break\n",
    "        else:\n",
    "            output_idx.append(int(pred[-1]))\n",
    "#             output.append(int(pred.argmax(dim=1)[-1]))\n",
    "    return(''.join([idx_to_word[i] for i in output_idx[1:]]), [idx_to_word[i] for i in output_idx[1:]], attentions[:t+1].t(), output_idx[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "test = [1, 15, 23]\n",
    "test = np.array(test, dtype=np.int64)\n",
    "mm = np.float32(test != 0)\n",
    "mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(topics, num_chars, model, idx_to_word, word_to_idx, is_sample=False):\n",
    "    output_idx = [1]\n",
    "    topics = [word_to_idx[x] for x in topics]\n",
    "    topics = torch.tensor(topics)\n",
    "    topics = topics.reshape((1, topics.shape[0]))\n",
    "#     hidden = torch.zeros(num_layers, 1, hidden_dim)\n",
    "#     hidden = (torch.zeros(num_layers, 1, hidden_dim).to(device), torch.zeros(num_layers, 1, hidden_dim).to(device))\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    if use_gpu:\n",
    "#         hidden = hidden.cuda()\n",
    "        adaptive_softmax.to(device)\n",
    "        topics = topics.to(device)\n",
    "        seq_length = torch.tensor(50).reshape(1, 1).to(device)\n",
    "    \"\"\"1\"\"\"    \n",
    "    coverage_vector = model.init_coverage_vector(topics.shape[0], topics.shape[1])\n",
    "    attentions = torch.zeros(num_chars, topics.shape[1])\n",
    "    X = torch.tensor(output_idx[-1]).reshape((1, 1)).to(device)\n",
    "    output = torch.zeros(1, hidden_dim).to(device)\n",
    "    log_prob, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, \n",
    "                                                                   topics=topics, \n",
    "                                                                   output=output, \n",
    "                                                                   hidden=hidden, \n",
    "                                                                   coverage_vector=coverage_vector, \n",
    "                                                                   seq_length=seq_length)\n",
    "    log_prob = log_prob.cpu().detach().reshape(-1).numpy()\n",
    "#     print(log_prob[10])\n",
    "    \"\"\"2\"\"\"\n",
    "    if is_sample:\n",
    "        top_indices = np.random.choice(vocab_size, beam_size, replace=False, p=np.exp(log_prob))\n",
    "    else:\n",
    "        top_indices = np.argsort(-log_prob)\n",
    "    \"\"\"3\"\"\"\n",
    "    beams = [(0.0, [idx_to_word[1]], idx_to_word[1], torch.zeros(1, topics.shape[1]), torch.ones(1, topics.shape[1]))]\n",
    "    b = beams[0]\n",
    "    beam_candidates = []\n",
    "#     print(attn_weight[0].cpu().data, coverage_vector)\n",
    "#     assert False\n",
    "    for i in range(beam_size):\n",
    "        word_idx = top_indices[i]\n",
    "        beam_candidates.append((b[0]+log_prob[word_idx], b[1]+[idx_to_word[word_idx]], word_idx, torch.cat((b[3], attn_weight[0].cpu().data), 0), torch.cat((b[4], coverage_vector.cpu().data), 0), hidden, output.squeeze(0), coverage_vector))\n",
    "    \"\"\"4\"\"\"\n",
    "    beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n",
    "    beams = beam_candidates[:beam_size] # truncate to get new beams\n",
    "    \n",
    "    for xy in range(num_chars-1):\n",
    "        beam_candidates = []\n",
    "        for b in beams:\n",
    "            \"\"\"5\"\"\"\n",
    "            X = torch.tensor(b[2]).reshape((1, 1)).to(device)\n",
    "            \"\"\"6\"\"\"\n",
    "            log_prob, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, \n",
    "                                                                           topics=topics, \n",
    "                                                                           output=b[6], \n",
    "                                                                           hidden=b[5], \n",
    "                                                                           coverage_vector=b[7], \n",
    "                                                                           seq_length=seq_length)\n",
    "            log_prob = log_prob.cpu().detach().reshape(-1).numpy()\n",
    "            \"\"\"8\"\"\"\n",
    "            if is_sample:\n",
    "                top_indices = np.random.choice(vocab_size, beam_size, replace=False, p=np.exp(log_prob))\n",
    "            else:\n",
    "                top_indices = np.argsort(-log_prob)\n",
    "            \"\"\"9\"\"\"\n",
    "            for i in range(beam_size):\n",
    "                word_idx = top_indices[i]\n",
    "                beam_candidates.append((b[0]+log_prob[word_idx], b[1]+[idx_to_word[word_idx]], word_idx, torch.cat((b[3], attn_weight[0].cpu().data), 0), torch.cat((b[4], coverage_vector.cpu().data), 0), hidden, output.squeeze(0), coverage_vector))\n",
    "        \"\"\"10\"\"\"\n",
    "        beam_candidates.sort(key = lambda x:x[0], reverse = True) # decreasing order\n",
    "        beams = beam_candidates[:beam_size] # truncate to get new beams\n",
    "    \n",
    "    \"\"\"11\"\"\"\n",
    "    if '<EOS>' in beams[0][1]:\n",
    "        first_eos = beams[0][1].index('<EOS>')\n",
    "    else:\n",
    "        first_eos = num_chars-1\n",
    "    return(''.join(beams[0][1][:first_eos]), beams[0][1][:first_eos], beams[0][3][:first_eos].t(), beams[0][4][:first_eos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.switch_backend('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "    \n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.subplots(1)\n",
    "#     cmap = 'bone'\n",
    "    cmap = 'viridis'\n",
    "    cax = ax.matshow(attentions.numpy(), cmap=cmap)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_yticklabels([''] + input_sentence.split(' '), fontproperties=fontprop, fontsize=10)\n",
    "    ax.set_xticklabels([''] + output_words, fontproperties=fontprop, fontsize=10, rotation=45)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    word_size = 0.5\n",
    "    fig.set_figheight(word_size * len(input_sentence.split(' ')))\n",
    "    fig.set_figwidth(word_size * len(output_words))\n",
    "    plt.show()\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence, method='beam_search', is_sample=False):\n",
    "    if method == 'beam_search':\n",
    "        _, output_words, attentions, coverage_vector = beam_search(input_sentence, 100, model, idx_to_word, word_to_idx, is_sample=is_sample)\n",
    "    else:\n",
    "        _, output_words, attentions, _ = predict_rnn(input_sentence, 100, model, idx_to_word, word_to_idx)\n",
    "    print('input =', ' '.join(input_sentence))\n",
    "    print('output =', ' '.join(output_words))\n",
    "#     n_digits = 3\n",
    "#     coverage_vector = torch.round(coverage_vector * 10**n_digits) / (10**n_digits)\n",
    "#     coverage_vector=np.round(coverage_vector, n_digits)\n",
    "#     print(coverage_vector.numpy())\n",
    "    showAttention(' '.join(input_sentence), output_words, attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bleu score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_bleu(model, topics_test, corpus_test, num_test, method='beam_search', is_sample=False):\n",
    "    bleu_2_score = 0\n",
    "    for i in tqdm(range(len(corpus_test[:num_test]))):\n",
    "        if method == 'beam_search':\n",
    "            _, output_words, _, _ = beam_search([idx_to_word[x] for x in topics_test[i]], 100, model, idx_to_word, word_to_idx, False)\n",
    "        else:\n",
    "            _, output_words, _, _ = predict_rnn([idx_to_word[x] for x in topics_test[i]], 100, model, idx_to_word, word_to_idx)\n",
    "        bleu_2_score += sentence_bleu([[idx_to_word[x] for x in corpus_test[i] if x not in [0, 2]]], output_words, weights=(0, 1, 0, 0))\n",
    "        \n",
    "    bleu_2_score = bleu_2_score / num_test * 100\n",
    "    return bleu_2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 512\n",
    "lr = 1e-3 * 0.5\n",
    "momentum = 0.01\n",
    "num_epoch = 100\n",
    "clip_value = 0.1\n",
    "use_gpu = True\n",
    "num_layers = 2\n",
    "bidirectional = False\n",
    "batch_size = 32\n",
    "num_keywords = 5\n",
    "verbose = 1\n",
    "check_point = 5\n",
    "beam_size = 2\n",
    "is_sample = True\n",
    "vocab_size = len(vocab)\n",
    "# device = torch.device(deviceName)\n",
    "loss_function = nn.NLLLoss()\n",
    "adaptive_softmax = nn.AdaptiveLogSoftmaxWithLoss(\n",
    "    1000, len(vocab), cutoffs=[round(vocab_size / 20), 4*round(vocab_size / 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"embedding_dim\": 100, \n",
    "#     \"hidden_dim\": 512, \n",
    "#     \"batch_size\": 64, \n",
    "#     \"num_keywords\": 5, \n",
    "#     \"lr\": 1e-3 * 0.5, \n",
    "#     \"momentum\": 0.01, \n",
    "#     \"num_epoch\": 100, \n",
    "#     \"clip_value\": 1, \n",
    "#     \"use_gpu\": True, \n",
    "#     \"num_layers\": 1, \n",
    "#     \"bidirectional\": False, \n",
    "#     \"verbose\": 1, \n",
    "#     \"check_point\": 5, \n",
    "#     \"beam_size\": 2, \n",
    "#     \"is_sample\": True, \n",
    "#     \"vocab_size\": len(vocab), \n",
    "#     \"device\": torch.device(deviceName)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/LiyuanLucasLiu/RAdam\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n",
    "            for param in params:\n",
    "                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n",
    "                    param['buffer'] = [[None, None, None] for _ in range(10)]\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = group['buffer'][int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    elif self.degenerated_to_sgd:\n",
    "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = -1\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif step_size > 0:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class PlainRAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "                    \n",
    "        self.degenerated_to_sgd = degenerated_to_sgd\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        super(PlainRAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(PlainRAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                beta2_t = beta2 ** state['step']\n",
    "                N_sma_max = 2 / (1 - beta2) - 1\n",
    "                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "                elif self.degenerated_to_sgd:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "                    p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class AdamW(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        \n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, warmup = warmup)\n",
    "        super(AdamW, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AdamW, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                \n",
    "                if group['warmup'] > state['step']:\n",
    "                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n",
    "                else:\n",
    "                    scheduled_lr = group['lr']\n",
    "\n",
    "                step_size = scheduled_lr * math.sqrt(bias_correction2) / bias_correction1\n",
    "                \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n",
    "\n",
    "                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n",
      "Dump to cuda\n"
     ]
    }
   ],
   "source": [
    "model = MTALSTM(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_keywords=num_keywords, \n",
    "                num_layers=num_layers, num_labels=len(vocab), weight=word_vec, bidirectional=bidirectional)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# from radam import RAdam\n",
    "optimizer = RAdam(model.parameters(), lr=lr)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2, min_lr=1e-7, verbose=True)\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "if use_gpu:\n",
    "#     model = nn.DataParallel(model)\n",
    "#     model = model.to(device)\n",
    "    # model = model.to('cuda:1')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print (device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"Dump to cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MTALSTM(\n",
       "  (embedding): Embedding(48878, 100)\n",
       "  (Uf): Linear(in_features=500, out_features=5, bias=False)\n",
       "  (decoder): AttentionDecoder(\n",
       "    (attention): Attention(\n",
       "      (Ua): Linear(in_features=100, out_features=512, bias=False)\n",
       "      (Wa): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (va): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (rnn): LSTM(200, 512, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (adaptiveSoftmax): AdaptiveLogSoftmaxWithLoss(\n",
       "    (head): Linear(in_features=512, out_features=2446, bias=False)\n",
       "    (tail): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=128, bias=False)\n",
       "        (1): Linear(in_features=128, out_features=7332, bias=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=32, bias=False)\n",
       "        (1): Linear(in_features=32, out_features=39102, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 126
    }
   ],
   "source": [
    "def params_init_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        y = 0.04\n",
    "        nn.init.uniform_(m.weight, -y, y)\n",
    "        \n",
    "model.apply(params_init_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ver.0 doesn't exist\n"
     ]
    }
   ],
   "source": [
    "version_num = 0\n",
    "# Type = 'best'\n",
    "Type = 'trainable'\n",
    "model_check_point = '%s/model_%s_%d.pk' % (save_folder, Type, version_num)\n",
    "optim_check_point = '%s/optim_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "loss_check_point = '%s/loss_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "epoch_check_point = '%s/epoch_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "bleu_check_point = '%s/bleu_%s_%d.pkl' % (save_folder, Type, version_num)\n",
    "loss_values = []\n",
    "epoch_values = []\n",
    "bleu_values = []\n",
    "if os.path.isfile(model_check_point):\n",
    "    print('Loading previous status (ver.%d)...' % version_num)\n",
    "    model.load_state_dict(torch.load(model_check_point, map_location='cpu'))\n",
    "    model = model.to(device)\n",
    "    optimizer.load_state_dict(torch.load(optim_check_point))\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.4, patience=2, min_lr=1e-7, verbose=True)\n",
    "    loss_values = torch.load(loss_check_point)\n",
    "    epoch_values = torch.load(epoch_check_point)\n",
    "    bleu_values = torch.load(bleu_check_point)\n",
    "    print('Load successfully')\n",
    "else:\n",
    "    print(\"ver.%d doesn't exist\" % version_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-1843a786d89c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'republican'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'administrations'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'health'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'care'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'political'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trumps'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'insurance'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'states'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'huge'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'leaving'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'<UNK>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'beam_search'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-118-54bdfb19faf5>\u001b[0m in \u001b[0;36mevaluateAndShowAttention\u001b[1;34m(input_sentence, method, is_sample)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluateAndShowAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'beam_search'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'beam_search'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattentions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoverage_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_sample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattentions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-117-c281bffbba5e>\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(topics, num_chars, model, idx_to_word, word_to_idx, is_sample)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     log_prob, output, hidden, attn_weight, coverage_vector = model.inference(inputs=X, \n\u001b[0m\u001b[0;32m     20\u001b[0m                                                                    \u001b[0mtopics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                                                    \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-113-7c0fde1b1ca5>\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, inputs, topics, output, hidden, mask, coverage_vector, seq_length)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mphi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_embed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_embed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mqueries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(['republican', 'administrations', 'health', 'care', 'political', 'trumps', 'insurance', 'states', 'huge', 'leaving', '<UNK>'], method='beam_search', is_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def isnan(x):\n",
    "    return x != x\n",
    "\n",
    "for name, p in model.named_parameters():\n",
    "#     if p.grad is None:\n",
    "#         continue\n",
    "    if p.requires_grad:\n",
    "        print(name, p)\n",
    "#         p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_lr(optimizer, epoch, factor=0.1, lr_decay_epoch=60):\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * factor\n",
    "        print('lr decayed to %.4f' % optimizer.param_group[0]['lr'])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "autograd.set_detect_anomaly(False)\n",
    "prev_epoch = 0 if not epoch_values else epoch_values[-1]\n",
    "best_bleu = 0 if not bleu_values else max(bleu_values)\n",
    "\n",
    "for epoch in range(num_epoch - prev_epoch):\n",
    "    epoch += prev_epoch\n",
    "    start = time.time()\n",
    "    num, total_loss = 0, 0\n",
    "#     optimizer = decay_lr(optimizer=optimizer, epoch=epoch+1)\n",
    "    topics_indice, corpus_indice = shuffleData(topics_indice, corpus_indice) # shuffle data at every epoch\n",
    "    data = data_iterator(corpus_indice, topics_indice, batch_size, max(length) + 1)\n",
    "    hidden = model.init_hidden(batch_size=batch_size)\n",
    "    weight = torch.ones(len(vocab))\n",
    "    weight[0] = 0\n",
    "    num_iter = len(corpus_indice) // batch_size\n",
    "    for X, Y, mask, topics in tqdm(data, total=num_iter):\n",
    "        num += 1\n",
    "#         hidden.detach_()\n",
    "        if use_gpu:\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            mask = mask.to(device)\n",
    "            topics = topics.to(device)\n",
    "#             hidden = hidden.to(device)\n",
    "#             hidden[0].to(device)\n",
    "#             hidden[1].to(device)\n",
    "            loss_function = loss_function.to(device)\n",
    "            weight = weight.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # init hidden layer\n",
    "#         hidden = model.init_hidden(num_layers, batch_size, hidden_dim)\n",
    "        coverage_vector = model.init_coverage_vector(batch_size, num_keywords)\n",
    "        init_output = torch.zeros(batch_size, hidden_dim).to(device)\n",
    "        # inputs, topics, output, hidden=None, mask=None, target=None, coverage_vector=None, seq_length=None):\n",
    "        output, _, hidden, _, _ = model(inputs=X, topics=topics, output=init_output, hidden=hidden, mask=mask, target=Y, coverage_vector=coverage_vector)\n",
    "#         output, hidden = model(X, topics)\n",
    "        hidden[0].detach_()\n",
    "        hidden[1].detach_()\n",
    "        \n",
    "        loss = (-output.output).reshape((-1, batch_size)).t() * mask\n",
    "#         loss = loss.sum(dim=1)\n",
    "        loss = loss.sum(dim=1) / mask.sum(dim=1)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        norm = 0.0\n",
    "#         norm = nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        nn.utils.clip_grad_value_(model.parameters(), 1)\n",
    "            \n",
    "        optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "        \n",
    "        if np.isnan(total_loss):\n",
    "            for name, p in model.named_parameters():\n",
    "                if p.grad is None:\n",
    "                    continue \n",
    "                print(name, p)\n",
    "            assert False, \"Gradient explode\"\n",
    "    \n",
    "    one_iter_loss = np.mean(total_loss)\n",
    "    lr_scheduler.step(one_iter_loss)\n",
    "#     print(\"One iteration loss {:.3f}\".format(one_iter_loss))\n",
    "    \n",
    "    # validation\n",
    "    bleu_score = 0\n",
    "    num_test = 500\n",
    "    bleu_score = evaluate_bleu(model, topics_test, corpus_test, num_test=num_test, method='predict_rnn', is_sample=False)\n",
    "    \n",
    "    bleu_values.append(bleu_score)\n",
    "    loss_values.append(total_loss / num)\n",
    "    epoch_values.append(epoch+1)\n",
    "    \n",
    "    # save checkpoint\n",
    "    if ((epoch + 1) % check_point == 0) or (epoch == (num_epoch - 1)) or epoch+1 > 90 or bleu_score > 4:\n",
    "        model_check_point = '%s/model_trainable_%d.pk' % (save_folder, epoch+1)\n",
    "        optim_check_point = '%s/optim_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        loss_check_point = '%s/loss_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        epoch_check_point = '%s/epoch_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        bleu_check_point = '%s/bleu_trainable_%d.pkl' % (save_folder, epoch+1)\n",
    "        torch.save(model.state_dict(), model_check_point)\n",
    "        torch.save(optimizer.state_dict(), optim_check_point)\n",
    "        torch.save(loss_values, loss_check_point)\n",
    "        torch.save(epoch_values, epoch_check_point)\n",
    "        torch.save(bleu_values, bleu_check_point)\n",
    "    \n",
    "    # save current best result\n",
    "    if bleu_score > best_bleu:\n",
    "        best_bleu = bleu_score\n",
    "        print('current best bleu: %.4f' % best_bleu)\n",
    "        model_check_point = '%s/model_best_%d.pk' % (save_folder, epoch+1)\n",
    "        optim_check_point = '%s/optim_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        loss_check_point = '%s/loss_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        epoch_check_point = '%s/epoch_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        bleu_check_point = '%s/bleu_best_%d.pkl' % (save_folder, epoch+1)\n",
    "        torch.save(model.state_dict(), model_check_point)\n",
    "        torch.save(optimizer.state_dict(), optim_check_point)\n",
    "        torch.save(loss_values, loss_check_point)\n",
    "        torch.save(epoch_values, epoch_check_point)\n",
    "        torch.save(bleu_values, bleu_check_point)\n",
    "        \n",
    "    # calculate time\n",
    "    end = time.time()\n",
    "    s = end - since\n",
    "    h = math.floor(s / 3600)\n",
    "    m = s - h * 3600\n",
    "    m = math.floor(m / 60)\n",
    "    s -= (m * 60 + h * 3600)\n",
    "    \n",
    "    # verbose\n",
    "    if ((epoch + 1) % verbose == 0) or (epoch == (num_epoch - 1)):\n",
    "        print('epoch %d/%d, loss %.4f, norm %.4f, predict bleu: %.4f, time %.3fs, since %dh %dm %ds'\n",
    "              % (epoch + 1, num_epoch, total_loss / num, norm, bleu_score, end - start, h, m, s))\n",
    "        \n",
    "        evaluateAndShowAttention(['現在', '未來', '夢想', '科學', '文化'], method='beam_search', is_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_values, loss_values, 'b')\n",
    "plt.title('loss vs. epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_values, bleu_values, 'r')\n",
    "plt.title('bleu vs. epoch')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('bleu score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = 5000\n",
    "bleu_score = evaluate_bleu(model, topics_test, corpus_test, num_test=num_test, method='predict_rnn', is_sample=False)\n",
    "bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(['媽媽', '希望', '長大', '孩子', '母愛'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['現在', '未來', '夢想', '科學', '文化'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['春天', '來臨', '田野', '聆聽', '小路'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['子女', '父母', '父愛', '無比', '溫暖'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['信念', '人生', '失落', '心靈', '不屈'], method='beam_search', is_sample=True)\n",
    "evaluateAndShowAttention(['體會', '母親', '滴水之恩', '母愛', '養育之恩'], method='beam_search', is_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}